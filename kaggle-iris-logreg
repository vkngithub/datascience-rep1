#Iris.csv => used the original public dataset and ran a simple logistic regression on it using gradient descent.
#Because there are 3 classes (Iris-setosa,Iris-versicolor,Iris-virginica), we can use one-vs-all multiclass classification 
#by testing model accuracy in several algorithms.

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

# Import Dependencies
%matplotlib inline

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
import seaborn as sns
import missingno

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.naive_bayes import GaussianNB 
from sklearn.naive_bayes import MultinomialNB

# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory

import os
print(os.listdir("../input"))

# Any results you write to the current directory are saved as output.
data = pd.read_csv('../input/Iris.csv')
#data.head()
#data.describe()
#data.head()
#data.tail()
#data.corr()
#plt.scatter(data.SepalLengthCm, data.SepalWidthCm)
#plt.xlabel("SepalLength")
#plt.ylabel("SepalWidth")
#plt.show()
#data.shape

#plot of missing values
missingno.matrix(data, figsize=(10,5))

#sns.boxplot(data=data,width=0.5,fliersize=5)
#sns.set(rc={'figure.figsize':(2,5)})

#split data into train and test
X=data.values[:,0:4]
y=data.values[:,4].astype(np.int)#using astype() method to convert array y's datatype, else we get the "ValueError: Unknown label type"
print("data X:", X.shape, "data y:", y.shape)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)#train/test split % is usually around 80/20 or 70/30.
#using random_state=42 everytime, you will get the same output the first time you make the split
#X_train
#print("X_test:",X_test)
#y_train
#y_test
print("Xtrain:", X_train.shape, "ytrain:", y_train.shape)
print("Xtest:", X_test.shape, "ytest:", y_test.shape)
print('-' * 50)

#function used by all models below
def fitmodel(modelobj,modeltype):
    #Fit the classifier to the data
    modelobj.fit(X_train,y_train)
    y_predictions=modelobj.predict(X_test)
    
    #check accuracy of our model on the test data
    print(modeltype,"Test accuracy is:",str(100*round(accuracy_score(y_test, y_predictions),2)),"%")
    
    # how did our model perform?
    count_misclassified = (y_test != y_predictions).sum()
    print('Misclassified samples: {}'.format(count_misclassified))
    
    #Confusion Matrix is a performance measurement for machine learning classification 
    #problem where output can be two or more classes. It is a table with different combinations of predicted and actual values.
    labels = ['Setosa class0','Virginica class1','Versicolor class2']
    conf_matrix = confusion_matrix(y_test, y_predictions)
    print('Confusion Matrix:')
    print(conf_matrix)
    print(classification_report(y_test, y_predictions,target_names=labels))
    
    ##plot the model
    #plt.scatter(y_test, y_predictions)
    #plt.xlabel("True Values1")
    #plt.ylabel("Predictions1")
    print('-' * 50)

#try diff models below for accuracy

#K nearest neighbor model
#Create KNN classifier
knn = KNeighborsClassifier(n_neighbors=3)
fitmodel(knn,"KNeighborsClassifier")

#logistic regression model
logreg= LogisticRegression(solver='lbfgs',multi_class='multinomial',max_iter=1500)
#multiclass='multinomial' is supported by ‘newton-cg’, ‘lbfgs’, ‘sag’, ‘saga’
fitmodel(logreg,"Logistic Regression")

#SVM model
svm= SVC(gamma='scale')
fitmodel(svm,"SVC")

#Gaussian Naive Bayes classifier
gnb = GaussianNB()
fitmodel(gnb,"Gaussian Naive Bayes")

#Multinomial Naive Bayes classifier
mnb = MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)
fitmodel(mnb,"Multinomial Naive Bayes")

o/p

['Iris.csv', 'database.sqlite']
data X: (150, 4) data y: (150,)
Xtrain: (120, 4) ytrain: (120,)
Xtest: (30, 4) ytest: (30,)
--------------------------------------------------
KNeighborsClassifier Test accuracy is: 90.0 %
Misclassified samples: 3
Confusion Matrix:
[[10  0  0]
 [ 0 11  0]
 [ 0  3  6]]
                   precision    recall  f1-score   support

    Setosa class0       1.00      1.00      1.00        10
 Virginica class1       0.79      1.00      0.88        11
Versicolor class2       1.00      0.67      0.80         9

        micro avg       0.90      0.90      0.90        30
        macro avg       0.93      0.89      0.89        30
     weighted avg       0.92      0.90      0.90        30

--------------------------------------------------
Logistic Regression Test accuracy is: 83.0 %
Misclassified samples: 5
Confusion Matrix:
[[10  0  0]
 [ 0 11  0]
 [ 0  5  4]]
                   precision    recall  f1-score   support

    Setosa class0       1.00      1.00      1.00        10
 Virginica class1       0.69      1.00      0.81        11
Versicolor class2       1.00      0.44      0.62         9

        micro avg       0.83      0.83      0.83        30
        macro avg       0.90      0.81      0.81        30
     weighted avg       0.89      0.83      0.82        30

--------------------------------------------------
SVC Test accuracy is: 70.0 %
Misclassified samples: 9
Confusion Matrix:
[[10  0  0]
 [ 0 11  0]
 [ 0  9  0]]
                   precision    recall  f1-score   support

    Setosa class0       1.00      1.00      1.00        10
 Virginica class1       0.55      1.00      0.71        11
Versicolor class2       0.00      0.00      0.00         9

        micro avg       0.70      0.70      0.70        30
        macro avg       0.52      0.67      0.57        30
     weighted avg       0.54      0.70      0.59        30

--------------------------------------------------
Gaussian Naive Bayes Test accuracy is: 100.0 %
Misclassified samples: 0
Confusion Matrix:
[[10  0  0]
 [ 0 11  0]
 [ 0  0  9]]
                   precision    recall  f1-score   support

    Setosa class0       1.00      1.00      1.00        10
 Virginica class1       1.00      1.00      1.00        11
Versicolor class2       1.00      1.00      1.00         9

        micro avg       1.00      1.00      1.00        30
        macro avg       1.00      1.00      1.00        30
     weighted avg       1.00      1.00      1.00        30

--------------------------------------------------
Multinomial Naive Bayes Test accuracy is: 67.0 %
Misclassified samples: 10
Confusion Matrix:
[[ 9  1  0]
 [ 0 11  0]
 [ 0  9  0]]
                   precision    recall  f1-score   support

    Setosa class0       1.00      0.90      0.95        10
 Virginica class1       0.52      1.00      0.69        11
Versicolor class2       0.00      0.00      0.00         9

        micro avg       0.67      0.67      0.67        30
        macro avg       0.51      0.63      0.54        30
     weighted avg       0.53      0.67      0.57        30

--------------------------------------------------
