# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 
import os
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
import missingno
import math, time, random, datetime
import warnings
import seaborn as sns

from numpy import nan as NA
from sklearn.model_selection import train_test_split
from sklearn import model_selection, metrics, linear_model
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler

warnings.filterwarnings('ignore')
%matplotlib inline

# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory

print(os.listdir("../input"))

# Any results you write to the current directory are saved as output.
train = pd.read_csv('../input/train.csv')
test = pd.read_csv('../input/test.csv')
sample_sub = pd.read_csv('../input/sample_submission.csv')
#train.head
#test.head
#sample_sub.head
#train.shape #(1460, 81)
#test.shape #(1459, 80)
#train.info()
#test.info() #SalePrice column is missing in test df
##missingno.matrix(train, figsize=(30,5))

#df1 = pd.DataFrame(np.random.randn(7, 3))
#df1
#df1.iloc[:3, 1] = NA #replace with NAN -> top 3 rows, column #1
#df1.iloc[:2, 0] = NA #replace with NAN -> top 2 rows, column #0
#df1

def find_missing_values(df, columns):
    """
    Finds number of rows where certain columns are missing values.
    ::param_df:: = target dataframe
    ::param_columns:: = list of columns
    """
    missing_vals = {}
    print("Number of missing or NaN values for each column:")
    df_length = len(df)
    for column in columns:
        total_column_isnotnull_values = df[column].value_counts().sum()
        total_column_isnull_values = df[column].isnull().sum()
        total_column_values = total_column_isnotnull_values + total_column_isnull_values
        #print(total_column_isnotnull_values)
        #print(total_column_isnull_values)
        #print(total_column_values)
        #print(df_length-total_column_isnotnull_values),2)
        #missing values and it's percentage
        missing_vals[column] = df_length-total_column_isnotnull_values,str(math.ceil(round((total_column_isnull_values/total_column_values)*100,2)))+"%"
    return missing_vals

train_missing_values = find_missing_values(train, columns=train.columns)
print("train missing values Before:", train_missing_values)

plt.scatter(train['LotArea'], train['SalePrice'], color='red')
plt.title('LotArea Vs SalePrice', fontsize=14)
plt.xlabel('LotArea', fontsize=14)
plt.ylabel('SalePrice', fontsize=14)
plt.grid(True)
plt.show()
 
plt.scatter(train['YearBuilt'], train['SalePrice'], color='blue')
plt.title('YearBuilt Vs SalePrice', fontsize=14)
plt.xlabel('YearBuilt', fontsize=14)
plt.ylabel('SalePrice', fontsize=14)
plt.grid(True)
plt.show()

plt.scatter(train['OverallQual'], train['SalePrice'], color='green')
plt.title('OverallQual Vs SalePrice', fontsize=14)
plt.xlabel('OverallQual', fontsize=14)
plt.ylabel('SalePrice', fontsize=14)
plt.grid(True)
plt.show()

plt.scatter(train['1stFlrSF'], train['SalePrice'], color='violet')
plt.title('1stFlrSF Vs SalePrice', fontsize=14)
plt.xlabel('1stFlrSF', fontsize=14)
plt.ylabel('SalePrice', fontsize=14)
plt.grid(True)
plt.show()

plt.scatter(train['2ndFlrSF'], train['SalePrice'], color='brown')
plt.title('2ndFlrSF Vs SalePrice', fontsize=14)
plt.xlabel('2ndFlrSF', fontsize=14)
plt.ylabel('SalePrice', fontsize=14)
plt.grid(True)
plt.show()

#sns.barplot(train.OverallQual,train.SalePrice)

train["Neighborhood"].value_counts() #categorical attribute shows diff neighborhoods and the no. of rows
print(train["Neighborhood"].value_counts())

train.hist(bins=50, figsize=(20,15))#histogram for each numerical attribute in train df
plt.show()

def handle_non_numerical_data(df):
    columns = df.columns.values
    for column in columns:
        text_digit_vals = {}
        
        def convert_to_int(val): #another function
            return text_digit_vals[val]

        if df[column].dtype != np.int64 and df[column].dtype != np.float64:
            #print("inside IF in func handle_non_numerical_data" , df.dtypes)
            #print("inside IF in func handle_non_numerical_data" , column)
            column_contents = df[column].values.tolist()
            unique_elements = set(column_contents)
            x = 0
            for unique in unique_elements:
                if unique not in text_digit_vals:
                    text_digit_vals[unique] = x
                    x+=1

            df[column] = list(map(convert_to_int, df[column]))
    return df

#convert Neighborhood col into numeric, else we get a ValueError
#mapping = {NAmes, CollgCr, OldTown, Edwards, Somerst, Gilbert, NridgHt, Sawyer , NWAmes , SawyerW, BrkSide, Crawfor, Mitchel, NoRidge, Timber , IDOTRR , ClearCr, SWISU, toneBr, 
           #MeadowV, Blmngtn, BrDale , Veenker, NPkVill, Blueste}

train_allnumeric = handle_non_numerical_data(train)#convert all non-numeric data into numeric data, hence new df 'train_new'

#train_allnumeric.hist(bins=50, figsize=(20,15))#histogram for each numerical attribute in train df
#plt.show()

#print(train_new.Neighborhood)
#print(train_new.head(1460))

#do fillna() on multiple columns at once
train_allnumeric = train_allnumeric.fillna(value=0)

print("Find most important features relative to target")
corr = train_allnumeric.corr()
print (corr['SalePrice'].sort_values(ascending=False)[:10], '\n') #+vely correlated
print (corr['SalePrice'].sort_values(ascending=False)[-10:]) #-vely correlated
 
print('train_allnumeric.SalePrice.describe(): ', train_allnumeric.SalePrice.describe())
#o/p of above is:
#count      1460.000000 | mean     180921.195890 | std       79442.502883 | min       34900.000000
#25%      129975.000000 | 50%      163000.000000 | 75%      214000.000000 | max      755000.000000
#interpretation: The average sale price of a house in our dataset is close to $180,000, 
#with most of the values falling within the $130,000 to $215,000 range.

#When performing regression, sometimes it makes sense to log-transform the target variable when it is skewed. 
#One reason for this is to improve the linearity of the data. 
plt.hist(train_allnumeric.SalePrice, color='blue')
plt.show()

y = np.log(train_allnumeric.SalePrice)
X = train_allnumeric.drop(['SalePrice', 'Id'], axis=1)

print ("Skew is:", y.skew())
plt.hist(y, color='red')
plt.show()

#splitting into training and cv for cross validation
#X = train_allnumeric.loc[:,['LotArea','Neighborhood','OverallQual','YearBuilt','1stFlrSF','GarageYrBlt','MoSold','YrSold']]
#X = train_allnumeric.loc[:,['LotArea','Neighborhood','OverallQual','YearBuilt','MoSold','YrSold','GrLivArea']]
#X = train_allnumeric.loc[:,['YearBuilt']]
#X_train, X_test, y_train, y_test = train_test_split(X,train_allnumeric.SalePrice, test_size=0.2, random_state=0)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=.2)

#perform feature-scaling (standardization)
stdsc = StandardScaler()
X_train_std = stdsc.fit_transform(X_train)
print(X_train_std)
X_test_std = stdsc.transform(X_test)

#--------------------------
# Function that runs the requested algorithm and returns the cv accuracy metrics
def fitmodel(modeltypeobj, X_train, y_train, X_test, y_test, cv):
    
    #cv is cross validation fold value
    print('-' * 50)
    start_time = time.time() #begin
    
    #model fit
    model = modeltypeobj.fit(X_train, y_train)
    #The r-squared value is a measure of how close the data are to the fitted regression line. 
    #It takes a value between 0 and 1, 1 meaning that all of the variance in the target is explained by the data. 
    #In general, a higher r-squared value means a better fit.
    print ("R^2 is: \n", model.score(X_test, y_test))
        
    # Cross Validation 
    # CV is a more robust metric because it is going through the train data several times(10 times a.k.a 10-fold)
    # and predict and repeat with diff versions of the same train data
      
    # Cross-validation accuracy metric -- this applies to classification problems only, so for regression 
    #problems, use RMSE to determine the accuracy of our model in predicting the target values.
    y_pred = model.predict(X_test)
    #negative R-squared simply means that the chosen model (with its constraints) fits the data really poorly
    #The RMSE measures the distance between our predicted values and actual values.
    rmse = mean_squared_error(y_test, y_pred)
    #print ('RMSE is: \n', rmse)
    
    end_time = (time.time() - start_time) #end
    
    #return train_pred, acc, acc_cv, start_time, end_time
    return y_pred, rmse, start_time, end_time

# Linear Regression
linmodel = linear_model.LinearRegression()
#train_pred, acc, acc_cv, start_time, end_time = fitmodel(linmodel, X_train, y_train, X_test, y_test, 10)
y_pred, rmse, start_time, end_time = fitmodel(linmodel, X_train_std, y_train, X_test_std, y_test, 10)
print("Linear Regression")
#print("Accuracy: %s" % acc)
print("Accuracy RMSE: %s" % rmse)
print("Running Time: %s" % datetime.timedelta(seconds=end_time))
print('-' * 50)

#If our predicted values were identical to the actual values, this graph would be a straight line
actual_values = y_test
plt.scatter(y_pred, actual_values, alpha=.7, color='b') #alpha helps to show overlapping data
plt.xlabel('Predicted Price')
plt.ylabel('Actual Price')
plt.title('Linear Regression Model')
plt.show()

######################

o/p:


['test.csv', 'train.csv', 'sample_submission.csv', 'data_description.txt']
Number of missing or NaN values for each column:
train missing values Before: {'Id': (0, '0%'), 'MSSubClass': (0, '0%'), 'MSZoning': (0, '0%'), 'LotFrontage': (259, '18%'), 'LotArea': (0, '0%'), 'Street': (0, '0%'), 'Alley': (1369, '94%'), 'LotShape': (0, '0%'), 'LandContour': (0, '0%'), 'Utilities': (0, '0%'), 'LotConfig': (0, '0%'), 'LandSlope': (0, '0%'), 'Neighborhood': (0, '0%'), 'Condition1': (0, '0%'), 'Condition2': (0, '0%'), 'BldgType': (0, '0%'), 'HouseStyle': (0, '0%'), 'OverallQual': (0, '0%'), 'OverallCond': (0, '0%'), 'YearBuilt': (0, '0%'), 'YearRemodAdd': (0, '0%'), 'RoofStyle': (0, '0%'), 'RoofMatl': (0, '0%'), 'Exterior1st': (0, '0%'), 'Exterior2nd': (0, '0%'), 'MasVnrType': (8, '1%'), 'MasVnrArea': (8, '1%'), 'ExterQual': (0, '0%'), 'ExterCond': (0, '0%'), 'Foundation': (0, '0%'), 'BsmtQual': (37, '3%'), 'BsmtCond': (37, '3%'), 'BsmtExposure': (38, '3%'), 'BsmtFinType1': (37, '3%'), 'BsmtFinSF1': (0, '0%'), 'BsmtFinType2': (38, '3%'), 'BsmtFinSF2': (0, '0%'), 'BsmtUnfSF': (0, '0%'), 'TotalBsmtSF': (0, '0%'), 'Heating': (0, '0%'), 'HeatingQC': (0, '0%'), 'CentralAir': (0, '0%'), 'Electrical': (1, '1%'), '1stFlrSF': (0, '0%'), '2ndFlrSF': (0, '0%'), 'LowQualFinSF': (0, '0%'), 'GrLivArea': (0, '0%'), 'BsmtFullBath': (0, '0%'), 'BsmtHalfBath': (0, '0%'), 'FullBath': (0, '0%'), 'HalfBath': (0, '0%'), 'BedroomAbvGr': (0, '0%'), 'KitchenAbvGr': (0, '0%'), 'KitchenQual': (0, '0%'), 'TotRmsAbvGrd': (0, '0%'), 'Functional': (0, '0%'), 'Fireplaces': (0, '0%'), 'FireplaceQu': (690, '48%'), 'GarageType': (81, '6%'), 'GarageYrBlt': (81, '6%'), 'GarageFinish': (81, '6%'), 'GarageCars': (0, '0%'), 'GarageArea': (0, '0%'), 'GarageQual': (81, '6%'), 'GarageCond': (81, '6%'), 'PavedDrive': (0, '0%'), 'WoodDeckSF': (0, '0%'), 'OpenPorchSF': (0, '0%'), 'EnclosedPorch': (0, '0%'), '3SsnPorch': (0, '0%'), 'ScreenPorch': (0, '0%'), 'PoolArea': (0, '0%'), 'PoolQC': (1453, '100%'), 'Fence': (1179, '81%'), 'MiscFeature': (1406, '97%'), 'MiscVal': (0, '0%'), 'MoSold': (0, '0%'), 'YrSold': (0, '0%'), 'SaleType': (0, '0%'), 'SaleCondition': (0, '0%'), 'SalePrice': (0, '0%')}



NAmes      225
CollgCr    150
OldTown    113
Edwards    100
Somerst     86
Gilbert     79
NridgHt     77
Sawyer      74
NWAmes      73
SawyerW     59
BrkSide     58
Crawfor     51
Mitchel     49
NoRidge     41
Timber      38
IDOTRR      37
ClearCr     28
StoneBr     25
SWISU       25
Blmngtn     17
MeadowV     17
BrDale      16
Veenker     11
NPkVill      9
Blueste      2
Name: Neighborhood, dtype: int64

Find most important features relative to target
SalePrice       1.000000
OverallQual     0.790982
GrLivArea       0.708624
GarageCars      0.640409
GarageArea      0.623431
TotalBsmtSF     0.613581
1stFlrSF        0.605852
FullBath        0.560664
TotRmsAbvGrd    0.533723
YearBuilt       0.522897
Name: SalePrice, dtype: float64 

KitchenAbvGr    -0.135907
Electrical      -0.142681
BsmtExposure    -0.193079
Neighborhood    -0.214913
Exterior1st     -0.233086
SaleCondition   -0.236484
Exterior2nd     -0.261249
Foundation      -0.441842
KitchenQual     -0.589189
ExterQual       -0.636884
Name: SalePrice, dtype: float64
train_allnumeric.SalePrice.describe():  count      1460.000000
mean     180921.195890
std       79442.502883
min       34900.000000
25%      129975.000000
50%      163000.000000
75%      214000.000000
max      755000.000000
Name: SalePrice, dtype: float64

Skew is: 0.12133506220520406

[[-0.8667643   0.40371455  0.35953495 ...  1.65006527 -0.3084789
   0.43940255]
 [ 0.07410996  0.40371455  0.04874271 ...  0.89367742 -0.3084789
   0.43940255]
 [-0.63154574  0.40371455  0.27477343 ...  0.13728958 -0.3084789
   0.43940255]
 ...
 [-0.8667643   0.40371455  0.07699655 ... -1.37548612 -0.3084789
   0.43940255]
 [-0.16110861  0.40371455 -0.06427265 ... -0.61909827 -0.3084789
   0.43940255]
 [ 1.48542135  0.40371455 -0.12078033 ...  0.89367742 -0.3084789
   0.43940255]]
--------------------------------------------------
R^2 is: 
 0.8671007628815164
Linear Regression
Accuracy RMSE: 0.024800794211141473
Running Time: 0:00:00.011587
--------------------------------------------------
